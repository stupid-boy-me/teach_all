import argparse
import os
import torch

import numpy as np
import cv2
from tqdm import tqdm
from torchvision import transforms
import onnxruntime as ort
# from scipy.signal import find_peaks
from skimage.draw import line




city_mean, city_std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]
transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(city_mean, city_std)])



def sample_lane_by_h_samples(lane_lines, h_samples, img_width=336):
    h_samples = np.array(h_samples, dtype=np.float64)
    
    for lane in lane_lines:
        x1, y1 = lane['start_point']  # (x, y)
        x2, y2 = lane['end_point']
        
        # è½¬ä¸º float é˜²æ­¢æ•´æ•°é™¤æ³•
        x1, y1, x2, y2 = float(x1), float(y1), float(x2), float(y2)
        
        # åˆå§‹åŒ– x ä¸º -2ï¼ˆæ— æ•ˆï¼‰
        x_values = np.full_like(h_samples, -2.0)
        
        # è®¡ç®— y èŒƒå›´
        y_min = min(y1, y2)
        y_max = max(y1, y2)
        
        # æ‰¾å‡ºåœ¨ [y_min, y_max] å†…çš„é‡‡æ ·è¡Œ
        valid_mask = (h_samples >= y_min) & (h_samples <= y_max)
        
        if np.any(valid_mask):
            y_targets = h_samples[valid_mask]
            
            # å¤„ç†æ°´å¹³çº¿ï¼ˆy1 == y2ï¼‰
            if abs(y2 - y1) < 1e-6:
                # æ°´å¹³çº¿ï¼šæ‰€æœ‰æœ‰æ•ˆ y å¯¹åº”åŒä¸€ä¸ª xï¼ˆå–ä¸­ç‚¹æˆ– x1ï¼‰
                x_interp = (x1 + x2) / 2.0
                x_values[valid_mask] = x_interp
            else:
                # æ ‡å‡†çº¿æ€§æ’å€¼ï¼šx = x1 + (x2 - x1) * (y - y1) / (y2 - y1)
                x_interp = x1 + (x2 - x1) * (y_targets - y1) / (y2 - y1)
                x_values[valid_mask] = x_interp
        
        # è£å‰ªåˆ°å›¾åƒå®½åº¦
        x_values = np.clip(x_values, 0, img_width - 1)
        x_values = np.round(x_values).astype(np.int32)
        
        # æ„å»º (x, y) ç‚¹
        sampled_points = [(int(x), int(y)) for x, y in zip(x_values, h_samples.astype(int))]
        lane['sampled_points'] = sampled_points
    
    return lane_lines


def PostProcess(instance_pred, input_feature_map_ratio, save_rows=True):
    """
    Args:
        instance_pred: numpy array (H, W, C), åµŒå…¥å‘é‡
        save_rows: ä¿ç•™ä¸¤æ¡è½¦é“çº¿çš„å‚æ•°
        input_feature_map_ratio : è¾“å…¥å’Œè¾“å‡ºç‰¹å¾å›¾çš„å€ç‡
    Returns:
        instance_mask: numpy array (H, W), å®ä¾‹åˆ†å‰²æ©ç  (0=èƒŒæ™¯, 1~4=ä¸åŒè½¦é“çº¿)
        lane_lines: list of dict, æ¯æ¡çº¿åŒ…å«:
            - 'start_point': (x, y) èµ·ç‚¹ï¼ˆå›¾åƒåº•éƒ¨ï¼‰
            - 'end_point': (x, y) ç»ˆç‚¹ï¼ˆå›¾åƒé¡¶éƒ¨ï¼‰
            - 'label': æ–°åˆ†é…çš„æ ‡ç­¾ (0,1,2,3)
            - 'points': åŸå§‹ç‚¹é›† (N,2) æ ¼å¼ä¸º (x, y)
    """
    # ğŸŸ¢ è½¬ä¸º torch.Tensor ä»¥ä¾¿å¤ç”¨ä½ åŸæœ‰çš„åå¤„ç†ï¼ˆä¹Ÿå¯çº¯ numpy æ”¹å†™ï¼‰
    instance_pred = torch.from_numpy(instance_pred)
    x_center = instance_pred.shape[3] * input_feature_map_ratio / 2.0
    # ğŸ‘‡ ä»¥ä¸‹å’Œä½ åŸæ¥ä»£ç å®Œå…¨ä¸€è‡´ ğŸ‘‡
    instance_map = torch.argmax(instance_pred, dim=1)  # shape: (1, H, W)
    instance_map = instance_map.squeeze(0).cpu().numpy()  # shape: (384, 672)

    
    labels = np.unique(instance_map)

    print(f"æ£€æµ‹åˆ°çš„å®ä¾‹ID: {labels}")

    label_counts = {}
    lane_lines = []  # å­˜å‚¨æ¯æ¡çº¿çš„ä¿¡æ¯
    for inst_id in labels:
        import random
        if inst_id == 0:  # è·³è¿‡èƒŒæ™¯
            continue
        mask = (instance_map == inst_id)
        if mask.sum() == 0:
            continue

        valid_coords = np.column_stack(np.where(mask))  # (row, col) = (v, u)

        # ADD
        valid_coords[:, 0] = valid_coords[:, 0] * input_feature_map_ratio  # x åæ ‡ä¹˜ä»¥å¸¸é‡
        valid_coords[:, 1] = valid_coords[:, 1] * input_feature_map_ratio  # y åæ ‡ä¹˜ä»¥å¸¸é‡

        label_counts[inst_id] = valid_coords
        if len(valid_coords) < 2:
            continue

        # æå– x, y
        ys = valid_coords[:, 0]  # è¡Œåæ ‡ï¼ˆé«˜åº¦æ–¹å‘ï¼‰
        xs = valid_coords[:, 1]  # åˆ—åæ ‡ï¼ˆå®½åº¦æ–¹å‘ï¼‰
        # å¤šé¡¹å¼æ‹Ÿåˆï¼ˆä¸€æ¬¡æ›²çº¿ = ç›´çº¿ï¼‰
        try:
            coeffs = np.polyfit(ys, xs, deg=1)  # æ‹Ÿåˆ x = f(y)
            poly = np.poly1d(coeffs)
        except:
            continue

        # å®šä¹‰é‡‡æ ·èŒƒå›´ï¼šä»å›¾åƒåº•éƒ¨åˆ°é¡¶éƒ¨
        y_min, y_max = int(np.min(ys)), int(np.max(ys))
        y_sample = np.linspace(y_min, y_max, 50)
        x_sample = poly(y_sample)

        # è¿‡æ»¤è¶…å‡ºå›¾åƒè¾¹ç•Œçš„ç‚¹  [å›¾åƒçš„è¾¹ç•Œä¹Ÿéœ€è¦è¿›è¡Œæ‰©å¤§, ä¸Šä¸€ä¸ªç‚¹è¿›è¡Œç¼©æ”¾åˆ°672 *384äº† å›¾åƒçš„è¾¹ç•Œä¹Ÿè¦è¿›è¡Œæ‰©å¤§]
        valid = (x_sample >= 0) & (x_sample < instance_pred.shape[3] * input_feature_map_ratio) & (y_sample >= 0) & (y_sample < instance_pred.shape[2] * input_feature_map_ratio)
        if not np.any(valid):
            continue
        x_sample = x_sample[valid]
        y_sample = y_sample[valid]

        if len(x_sample) < 2:
            continue

        # èµ·ç‚¹ = æœ€åº•éƒ¨ç‚¹ (yæœ€å¤§), ç»ˆç‚¹ = æœ€é¡¶éƒ¨ç‚¹ (yæœ€å°)
        start_point = (int(x_sample[-1]), int(y_sample[-1]))   # åº•éƒ¨
        end_point = (int(x_sample[0]), int(y_sample[0]))       # é¡¶éƒ¨

        # è®¡ç®—ä¸­ç‚¹ x åæ ‡ï¼ˆç”¨äºå·¦å³åˆ†ç»„ï¼‰
        mid_x = (start_point[0] + end_point[0]) / 2.0
        mid_y = (start_point[1] + end_point[1]) / 2
        lane_lines.append({
            'start_point': start_point,   # (x, y)
            'end_point': end_point,       # (x, y)
            'mid_x': mid_x,               # ä¸­ç‚¹ xï¼Œç”¨äºåˆ†ç»„
            'mid_y': mid_y,        # æœ€å° yï¼ˆé¡¶éƒ¨ç‚¹ yï¼‰ï¼Œç”¨äºæ’åº
            'points': valid_coords,
        })


    # ======== æ–°å¢ï¼šæŒ‰ä¸­ç‚¹åˆ’åˆ†å·¦å³ï¼Œå†æŒ‰ min_y æ’åºåˆ†é…æ ‡ç­¾ ========

    left_lines = []
    right_lines = []

    for line in lane_lines:
        if line['mid_x'] < x_center:
            left_lines.append(line)
        else:
            right_lines.append(line)

    # å·¦ä¾§ï¼šæŒ‰ min_y å‡åºæ’åºï¼ˆy è¶Šå°è¶Šé ä¸Šï¼‰
    left_lines.sort(key=lambda x: x['mid_y'], reverse = True)
    # å³ä¾§ï¼šæŒ‰ min_y å‡åºæ’åº
    right_lines.sort(key=lambda x: x['mid_y'], reverse = True)

    # åˆ†é…æ ‡ç­¾
    for i, line in enumerate(left_lines):
        line['label'] = 1 if i == 0 else 0  # ç¬¬ä¸€æ¡æ˜¯1ï¼Œå…¶ä½™æ˜¯0

    for i, line in enumerate(right_lines):
        line['label'] = 2 if i == 0 else 3  # ç¬¬ä¸€æ¡æ˜¯2ï¼Œå…¶ä½™æ˜¯3

    # åˆå¹¶å› lane_linesï¼ˆä¿æŒåŸå§‹é¡ºåºæˆ–æŒ‰æ ‡ç­¾æ’åºï¼‰
    lane_lines = left_lines + right_lines

    if save_rows:
        # åªä¿ç•™æ ‡ç­¾æ˜¯1å’Œ2çš„
        lane_lines = [lane for lane in lane_lines if lane['label'] in (1, 2)]
    
    tu_simple_h_samples = [240, 250, 260, 270, 280, 290, 300, 310, 320, 330, 340, 350, 360, 370, 380, 390, 400, 410, 420, 430, 440, 450, 460, 470, 480, 490, 500, 510, 520, 530, 540, 550, 560, 570, 580, 590, 600, 610, 620, 630, 640, 650, 660, 670, 680, 690, 700, 710]

    # åŸå›¾é«˜åº¦ 720 â†’ ä½ çš„å›¾é«˜åº¦ 384
    scale = instance_map.shape[0] * input_feature_map_ratio / 720.0
    samples = [int(round(h * scale )) for h in tu_simple_h_samples]

    # é‡‡æ ·
    result = sample_lane_by_h_samples(
        lane_lines=lane_lines,
        h_samples=samples,
        img_width=instance_map.shape[1] * input_feature_map_ratio
    )
    return result

# ========== æ›¿æ¢ load_model ==========

def load_onnx_model(onnx_path):
    # åˆ›å»ºæ¨ç†ä¼šè¯
    session = ort.InferenceSession(onnx_path, providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])
    return session



def preprocess_cv2_image(image):
    
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    # image = np.transpose(image, (2, 0, 1)).astype(np.float32) / 255.0  # HWC -> CHW, [0,1]

    # city_mean = np.array([0.485, 0.456, 0.406], dtype=np.float32).reshape(3, 1, 1)
    # city_std = np.array([0.229, 0.224, 0.225], dtype=np.float32).reshape(3, 1, 1)
    # image = (image - city_mean) / city_std

    image = (image.astype(np.float32) / 255.0 - (0.485, 0.456, 0.406)) / (
        0.229,
        0.224,
        0.225,
    )

    image = image.transpose(2, 0, 1).astype(np.float32) # [np.newaxis, ...]
    return image



def draw_vanishing_point_analysis(bgr_image, mask, start_ratio=0.5, 
                                  point_color=(0, 255, 255),      # é»„è‰²ï¼šè¾¹ç•Œç‚¹
                                  left_line_color=(255, 0, 0),    # è“è‰²ï¼šå·¦çº¿
                                  right_line_color=(0, 0, 255),   # çº¢è‰²ï¼šå³çº¿
                                  vp_color=(0, 255, 0),           # ç»¿è‰²ï¼šæ¶ˆå¤±ç‚¹
                                  line_thickness=2,
                                  point_radius=3):
    """
    åœ¨åŸå›¾ä¸Šç»˜åˆ¶ï¼š
    - å·¦å³è¾¹ç•Œç‚¹ï¼ˆä¸‹åŠéƒ¨åˆ†ï¼‰
    - æ‹Ÿåˆçš„å·¦å³è¾¹ç•Œå»¶é•¿çº¿
    - ä¸¤æ¡çº¿çš„äº¤ç‚¹ï¼ˆæ¶ˆå¤±ç‚¹ï¼‰

    Args:
        bgr_image: åŸå›¾ï¼ŒBGR æ ¼å¼ï¼Œnp.ndarray, shape (H, W, 3)
        mask: äºŒå€¼ maskï¼Œå€¼ä¸º 0 æˆ– 1ï¼Œshape (H, W)
        start_ratio: ä»å›¾åƒé«˜åº¦çš„ start_ratio å¼€å§‹æ‰«æï¼ˆå¦‚ 0.5ï¼‰
    
    Returns:
        result_image: å¸¦æœ‰å¯è§†åŒ–å…ƒç´ çš„å›¾åƒ
    """
    H, W = mask.shape
    result = bgr_image.copy()
    
    # Step 1: æ‰¾è¾¹ç•Œç‚¹
    start_row = int(H * start_ratio)
    left_points = []
    right_points = []

    for y in range(start_row, H):
        row = mask[y]
        if row.sum() == 0:
            break
        xs = np.where(row == 1)[0]
        left_x = xs[0]
        right_x = xs[-1]
        if abs(right_x - left_x) >= 50:
            left_points.append((int(left_x + 5), int(y)))
            right_points.append((int(right_x - 5), int(y)))
            # å¯é€‰ï¼šç»˜åˆ¶è¾¹ç•Œç‚¹
            cv2.circle(result, (left_x, y), point_radius, point_color, -1)
            cv2.circle(result, (right_x, y), point_radius, point_color, -1)

    if len(left_points) < 2 or len(right_points) < 2:
        print("Not enough points to fit lines.")
        return result

    # Step 2: æ‹Ÿåˆç›´çº¿
    left_pts = np.array(left_points, dtype=np.float32)
    right_pts = np.array(right_points, dtype=np.float32)

    left_line = cv2.fitLine(left_pts, cv2.DIST_L2, 0, 0.01, 0.01).flatten()  # [vx, vy, x0, y0]
    right_line = cv2.fitLine(right_pts, cv2.DIST_L2, 0, 0.01, 0.01).flatten()

    # # Step 3: è®¡ç®—äº¤ç‚¹
    def line_intersection(line1, line2):
        vx1, vy1, x1, y1 = line1
        vx2, vy2, x2, y2 = line2
        A = np.array([[vx1, -vx2], [vy1, -vy2]], dtype=np.float32)
        b = np.array([x2 - x1, y2 - y1], dtype=np.float32)
        if abs(np.linalg.det(A)) < 1e-6:
            return None
        t1, _ = np.linalg.solve(A, b)
        x = x1 + t1 * vx1
        y = y1 + t1 * vy1
        return (float(x), float(y))

    vp = line_intersection(left_line, right_line)

    # Step 4: ç»˜åˆ¶å»¶é•¿çº¿ï¼ˆè¦†ç›–æ•´å¼ å›¾åƒé«˜åº¦èŒƒå›´ï¼‰
    def draw_line(img, line, color, thickness):
        vx, vy, x0, y0 = line
        # ç”Ÿæˆä¸¤ä¸ªç«¯ç‚¹ï¼šè®©çº¿ç©¿è¿‡æ•´ä¸ªå›¾åƒï¼ˆä¸Šä¸‹è¾¹ç•Œï¼‰
        if abs(vx) < 1e-6:  # å‚ç›´çº¿
            pt1 = (int(x0), 0)
            pt2 = (int(x0), H)
        else:
            # y = y0 + (vy/vx)(x - x0)
            # å½“ y=0:
            x_top = x0 - y0 * vx / vy if abs(vy) > 1e-6 else x0
            # å½“ y=H-1:
            x_bottom = x0 + (H - 1 - y0) * vx / vy if abs(vy) > 1e-6 else x0
            pt1 = (int(x_top), 0)
            pt2 = (int(x_bottom), H - 1)
        cv2.line(img, pt1, pt2, color, thickness)
        return pt1, pt2

    draw_line(result, left_line, left_line_color, line_thickness)
    draw_line(result, right_line, right_line_color, line_thickness)

    # Step 5: ç»˜åˆ¶æ¶ˆå¤±ç‚¹ï¼ˆå³ä½¿åœ¨å›¾åƒå¤–ï¼Œä¹Ÿæ ‡åœ¨æœ€è¿‘è¾¹ç•Œæˆ–ç”»åå­—ï¼‰
    if vp is not None:
        x, y = vp
        if 0 <= x < W and 0 <= y < H:
            cv2.circle(result, (int(x), int(y)), 8, vp_color, -1)
            cv2.circle(result, (int(x), int(y)), 10, (0, 0, 0), 2)
        else:
            # åœ¨å›¾åƒè¾¹ç•Œä¸Šç”»ä¸€ä¸ªåå­—è¡¨ç¤ºæ–¹å‘
            cv2.drawMarker(result, (int(x), int(y)), vp_color, markerType=cv2.MARKER_CROSS,
                           markerSize=20, thickness=2)

    # å¯é€‰ï¼šç»˜åˆ¶æ£€æµ‹åˆ°çš„è¾¹ç•Œç‚¹ï¼ˆå–æ¶ˆæ³¨é‡Šä¸‹é¢ä¸¤è¡Œï¼‰
    # for pt in left_points:
    #     cv2.circle(result, pt, point_radius, point_color, -1)
    # for pt in right_points:
    #     cv2.circle(result, pt, point_radius, point_color, -1)

    return result, vp



def get_boundary_points(H, W, step=2):
    """
    æŒ‰é¡ºåºç”Ÿæˆè¾¹ç•Œç‚¹åˆ—è¡¨ï¼ˆæ”¯æŒæ­¥é•¿æ§åˆ¶ï¼‰ï¼š
    1. å·¦è¾¹ç•Œä¸‹åŠæ®µ: (x=0, y=H//2 â†’ H-1)      [ä»ä¸Šåˆ°ä¸‹]
    2. åº•è¾¹:         (y=H-1, x=0 â†’ W-1)       [ä»å·¦åˆ°å³]
    3. å³è¾¹ç•Œä¸‹åŠæ®µ: (x=W-1, y=H-1 â†’ H//2)    [ä»ä¸‹åˆ°ä¸Š]

    Args:
        H, W: å›¾åƒé«˜å®½
        step: é‡‡æ ·æ­¥é•¿ï¼ˆæ­£æ•´æ•°ï¼‰ï¼Œé»˜è®¤ä¸º1ï¼ˆå…¨é‡‡æ ·ï¼‰

    Returns:
        points: list of (row, col) = (y, x)
    """
    if step < 1:
        raise ValueError("step must be >= 1")

    points = []

    # 1. å·¦è¾¹ç•Œä¸‹åŠæ®µ: y from H//2 to H-1
    for y in range(H // 2, H, step):
        points.append((y, 0))

    # 2. åº•è¾¹: x from 0 to W-1
    y_bottom = H - 1
    for x in range(0, W, step):
        points.append((y_bottom, x))

    # 3. å³è¾¹ç•Œä¸‹åŠæ®µ: y from H-1 down to H//2
    x_right = W - 1
    # ä½¿ç”¨ range(start, stop, step)ï¼Œstart=H-1, stop=H//2-1, step=-step
    y = H - 1
    while y >= H // 2:
        points.append((y, x_right))
        y -= step

    return points

def count_ones_and_get_coordinates(y_vp_int, x_vp_int, y_p, x_p, mask):
    H, W = mask.shape
    # è·å– vp åˆ° p çš„æ‰€æœ‰åƒç´ åæ ‡
    rr, cc = line(y_vp_int, x_vp_int, y_p, x_p)  # rr=y, cc=x

    # è¿‡æ»¤æ‰è¶…å‡ºå›¾åƒè¾¹ç•Œçš„ç‚¹ï¼ˆç†è®ºä¸Šä¸ä¼šï¼Œä½†å®‰å…¨èµ·è§ï¼‰
    valid = (rr >= 0) & (rr < H) & (cc >= 0) & (cc < W)
    rr, cc = rr[valid], cc[valid]

    # ç»Ÿè®¡ mask[rr, cc] ä¸­ 1 çš„ä¸ªæ•°
    num_ones = np.sum(mask[rr, cc] == 1)
    
    # è·å– mask[rr, cc] ä¸­ 1 çš„åæ ‡
    ones_coordinates = np.column_stack((rr[mask[rr, cc] == 1], cc[mask[rr, cc] == 1]))

    return num_ones, ones_coordinates


def count_ones_along_lines_from_vp(mask, vp):
    """
    å¯¹æ¯ä¸ªè¾¹ç•Œç‚¹ pï¼Œè®¡ç®—çº¿æ®µ vp -> p ä¸Š mask ä¸­å€¼ä¸º 1 çš„åƒç´ ä¸ªæ•°ã€‚

    Args:
        mask: 2D np.ndarray, shape (H, W), values in {0, 1}
        vp: tuple (x, y) â€”â€” æ³¨æ„ï¼šæ˜¯ (x, y)ï¼Œå³ (col, row)

    Returns:
        counts: list of int, é•¿åº¦ = è¾¹ç•Œç‚¹æ•°é‡
    """
    H, W = mask.shape
    x_vp, y_vp = vp  # vp æ˜¯ (x, y)
    
    # è½¬ä¸ºæ•´æ•°åæ ‡ï¼ˆline è¦æ±‚æ•´æ•°ï¼‰
    x_vp_int = int(round(x_vp))
    y_vp_int = int(round(y_vp))

    boundary_points = get_boundary_points(H, W, 2)  # list of (row, col) = (y, x)
    counts = []
    coordinates = []
    for (y_p, x_p) in boundary_points:
        num_ones, ones_coordinates = count_ones_and_get_coordinates(y_vp_int, x_vp_int, y_p, x_p, mask)
        
        counts.append(int(num_ones))
        coordinates.append(ones_coordinates)
    return counts, coordinates, boundary_points


def find_all_significant_peaks(values, 
                               window_size=10,
                               rel_height_threshold=0.3,
                               min_peak_distance=20):
    """
    æ‰¾æ‰€æœ‰æ˜¾è‘—å³°å€¼çš„åŸå§‹ç´¢å¼•ã€‚
    
    Args:
        values: list or 1D array
        window_size: ç”¨äºä¼°è®¡å±€éƒ¨èƒŒæ™¯ï¼ˆå¯é€‰ï¼Œå½“å‰ç”¨äºåŠ¨æ€é˜ˆå€¼ï¼‰
        rel_height_threshold: å³°å€¼éœ€ >= global_max * rel_height_threshold
        min_peak_distance: å³°ä¹‹é—´æœ€å°è·ç¦»ï¼ˆé˜²å¯†é›†æ£€æµ‹ï¼‰
    
    Returns:
        peak_indices: list of int
    """
    values = np.array(values, dtype=np.float32)
    n = len(values)
    if n < 3:
        return []

    global_max = values.max()
    if global_max == 0:
        return []

    min_height = rel_height_threshold * global_max

    # Step 1: æ‰¾æ‰€æœ‰å±€éƒ¨æœ€å¤§å€¼ï¼ˆä¸¥æ ¼ï¼šå·¦<ä¸­>=å³ï¼‰
    local_max_indices = []
    for i in range(1, n - 1):
        if values[i] > values[i - 1] and values[i] >= values[i + 1]:
            if values[i] >= min_height:
                local_max_indices.append(i)

    # å¦‚æœæ²¡æœ‰ï¼Œå°è¯•æ”¾å®½æ¡ä»¶ï¼ˆ>= ä¸¤è¾¹ï¼‰
    if not local_max_indices:
        for i in range(1, n - 1):
            if values[i] >= values[i - 1] and values[i] >= values[i + 1]:
                if values[i] >= min_height:
                    local_max_indices.append(i)

    if not local_max_indices:
        return []

    # Step 2: æŒ‰é«˜åº¦é™åºæ’åºï¼Œç”¨äºè·ç¦»è¿‡æ»¤ï¼ˆè´ªå¿ƒä¿ç•™é«˜çš„ï¼‰
    local_max_indices = np.array(local_max_indices)
    heights = values[local_max_indices]
    sorted_idx = np.argsort(-heights)  # ä»é«˜åˆ°ä½
    sorted_peaks = local_max_indices[sorted_idx]

    # Step 3: è·ç¦»è¿‡æ»¤ï¼ˆä¿ç•™é«˜ä¸”ä¸è¿‘çš„ï¼‰
    final_peaks = []
    for peak in sorted_peaks:
        # æ£€æŸ¥æ˜¯å¦ä¸å·²é€‰å³°å¤ªè¿‘
        too_close = False
        for p in final_peaks:
            if abs(peak - p) < min_peak_distance:
                too_close = True
                break
        if not too_close:
            final_peaks.append(peak)

    # æŒ‰åŸå§‹é¡ºåºè¿”å›
    final_peaks.sort()
    return final_peaks

def select_peaks_with_side(final_peaks, coordinates,index):
    """
    è¿”å›å§‹ç»ˆåŒ…å« 'left' å’Œ 'right' é”®çš„å­—å…¸ã€‚
    æ— å¯¹åº”å³°å€¼æ—¶ï¼Œå€¼è®¾ä¸º -1ã€‚
    """
    # åˆå§‹åŒ–ç»“æœ
    result = {"left": -1, "right": -1}

    if not final_peaks:
        return result

    final_peaks = sorted(final_peaks)
    n = len(final_peaks)

    if n == 1:
        p = final_peaks[0]
        if p <= index:
            result["left"] = coordinates[p]
        else:
            result["right"] = coordinates[p]

    elif n == 2:
        p1, p2 = final_peaks
        result["left"] = coordinates[p1]
        result["right"] = coordinates[p2]

    else:  # n >= 3
        left_candidates = [p for p in final_peaks if p < index]
        right_candidates = [p for p in final_peaks if p > index]

        if left_candidates:
            result["left"] = coordinates[max(left_candidates)][::10]  # æœ€é è¿‘ index çš„å·¦ä¾§

        if right_candidates:
            result["right"] = coordinates[min(right_candidates)][::10]   # æœ€é è¿‘ index çš„å³ä¾§

    return result


# å›¾åƒæµ‹è¯•
if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Predict segmentation result from a given image')

    parser.add_argument('--model_weight', type=str, default='/algdata01/yiguo.huang/project_code/NextVpu/UFLDv2/liuxiao/FastSCNNSimpleInstanceSegENetCrossConvTestunsampleAugment_672_384_FDMobileNet/cpt_FDMobilenet_diceloss/FastSCNN_FDMobilenet_backbone_384_672_model-sim.onnx', help='Pretrained model weight')
    parser.add_argument('--input_pic', type=str, default='/algdata03/common/autodrive/saferider/lane2d/SegPoseLine/SegmentLine/Make_CVAT_æ•°æ®å ‚/20250201_002016_main_cvat1018/20250201_002016_main_cvat1018/images/20250201_002016_main_frame_07260.jpg', help='Path to the input picture')
    # /algdata03/common/autodrive/saferider/lane2d/SegPoseLine/SegmentLine/Make_CVAT_æ•°æ®å ‚/20250201_002016_main_cvat1018/20250201_002016_main_cvat1018/images/20250201_002016_main_frame_07260.jpg
    # args parse
    args = parser.parse_args()
    model_weight, input_pic = args.model_weight, args.input_pic
    # path_root = "/algdata03/common/autodrive/saferider/lane2d/SegPoseLine/SCNNMerge/leftImg8bit/val"
    # path_root = "/algdata03/common/autodrive/saferider/lane2d/SegPoseLine/SCNN/20250201_002016_main_cvat1018/images/val/"
    
    # path_root = "/algdata03/common/autodrive/saferider/lane2d/SegPoseLine/SCNN/20250328sel_imgs1200_1080p_cvat1014/images/train/"
    # one
    # path_root = "/algdata03/common/autodrive/saferider/lane2d/SegPoseLine/SCNN/20250415sel_imgs2120_1080p_cvat1015/images/train/"
    # cvat_basename = '20250415sel_imgs2120_1080p_cvat1015_0.5_augment'
    # two
    path_root = "/algdata01/yiguo.huang/project_code/NextVpu/UFLDv2/liuxiao/FastSCNNSimpleInstanceSegENetCrossConvTestunsampleAugment_672_384_FDMobileNet/test_images/20250201_001020_main-backward"
    path_root = "/algdata03/common/autodrive/saferider/lane2d/SegPoseLine/SegmentLine/Make_CVAT_æ•°æ®å ‚/20250201_002016_main_cvat1018/20250201_002016_main_cvat1018/images/"
    path_root = "/algdata03/common/autodrive/saferider/lane2d/SegPoseLine/SegmentLine/Make_CVAT_æ•°æ®å ‚/task_20251015_railway_2025_10_21_09_40_15_cvat1216/task_20251015_railway_2025_10_21_09_40_15_cvat1216/images/"
    cvat_basename = '20250201_001020_main-backward'
    args.save = os.path.join("/algdata01/yiguo.huang/project_code/NextVpu/UFLDv2/liuxiao/FastSCNNSimpleInstanceSegENetCrossConvTestunsampleAugment_672_384_FDMobileNet/vis_seg", cvat_basename)
    
    os.makedirs(args.save, exist_ok=True)
    filenames = [filename for filename in os.listdir(path_root) if "jpg" in filename]


    session = load_onnx_model(model_weight)

    for filename in tqdm(filenames[:10]):
        input_pic = os.path.join(path_root, filename)
        image = cv2.imread(input_pic)
        H_cv, W_cv = image.shape[:2]


        basename = os.path.basename(input_pic)
        # cv2.imwrite("/algdata01/yiguo.huang/project_code/NextVpu/UFLDv2/liuxiao/FastSCNNSimpleInstanceSegENetCrossConv18ms_nearestFull/vis_seg/{}_ori.jpg".format(basename),image_cv)
        # åœ¨672 * 384 ä¸Šè¿›è¡Œå¯è§†åŒ–
        filename1 = basename.split(".")[0]

        image_cv_resize = cv2.resize(image, (672, 384))  # (W, H)  # è¿™ä¸ªä¸éœ€è¦åŠ¨ å‰å¤„ç†çš„è¾“å…¥

        # âœ… é¢„å¤„ç†ï¼šcv2 â†’ numpy CHW float32
        input_tensor = preprocess_cv2_image(image_cv_resize)
        input_tensor = np.expand_dims(input_tensor, axis=0)  # (1, 3, 384, 672)

        # âœ… ONNX æ¨ç†
        instance_pred = session.run(
            ['instance_pred'],  # è¾“å‡ºåéœ€ä¸å¯¼å‡ºæ—¶ä¸€è‡´
            {'input': input_tensor}
        )[0]


        instance_pred = torch.from_numpy(instance_pred)

        # ğŸ‘‡ ä»¥ä¸‹å’Œä½ åŸæ¥ä»£ç å®Œå…¨ä¸€è‡´ ğŸ‘‡
        instance_map = torch.argmax(instance_pred, dim=1)  # shape: (1, H, W)
        instance_map = instance_map.squeeze(0).cpu().numpy()  # shape: (384, 672)
     
        # æ‰¾åˆ°ä¸¤ä¾§
        # å‡è®¾ä½ æœ‰ä¸€ä¸ª maskï¼Œshape=(480, 640)

        # image_cv_resize_vis, vp = draw_vanishing_point_analysis(image_cv_resize, instance_map, start_ratio=0.6)

        # values, coordinates, boundary_points = count_ones_along_lines_from_vp(instance_map, vp)

        # final_peaks = find_all_significant_peaks(values, 15)

        # mid_points = (int(instance_map.shape[1] / 2), 0)
        # index = boundary_points.index(mid_points)
        # final_lanes_points = select_peaks_with_side(final_peaks, coordinates, index)

        # '''
        pred = instance_map
        pred_255 = pred * 255
            
        # å°†å•é€šé“å›¾åƒè½¬æ¢ä¸ºBGRæ ¼å¼
        pred_255_bgr = cv2.cvtColor(pred_255.astype(np.uint8), cv2.COLOR_GRAY2BGR)
        # å°†maskå›¾è½¬æ¢ä¸ºé¢œè‰²æ˜ å°„ï¼Œä»¥ä¾¿æ›´å¥½åœ°å¯è§†åŒ–
        colormap = cv2.COLORMAP_JET
        mask_colored = cv2.applyColorMap(pred_255_bgr, colormap)
       
        # å°†å•é€šé“BGRå›¾åƒä¸åŸå§‹BGRå›¾åƒåˆå¹¶
        merged_image = cv2.addWeighted(image_cv_resize, 0.5, mask_colored, beta = 0.5 , gamma = 0 )


        cv2.imwrite(os.path.join(args.save, basename),merged_image)
        # cv2.imwrite(os.path.join(args.save, filename1 + "_vis.jpg"),image_cv_resize)
        # '''
        
        # for key, values in final_lanes_points.items():
        #     import random
        #     color_vis = (random.randint(0,255), random.randint(0,255), random.randint(0,255))
        #     for value in values:
        #         cv2.circle(image_cv_resize, (int(value[1]), int(value[0])), radius = 2, color=color_vis, thickness = 2)
            
       
        # cv2.imwrite(os.path.join(args.save, basename.split('.')[0] + "final_.jpg"),image_cv_resize)

        # assert False
       